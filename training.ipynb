{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b132cba1",
   "metadata": {},
   "source": [
    "# The Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0648fd1",
   "metadata": {},
   "source": [
    "I want to use the architecture encoder-decoder with attention architecture pattern:\n",
    "\n",
    "* Encode the image into a rich visual representation\n",
    "* Decode our representation into a sequence of words, one word at a time\n",
    "* Use attention to let the decoder focus on different image regions for each word\n",
    "\n",
    "Encoder will be a CNN with spatial feature map (7x7x512)\n",
    "Decoder will be an LSTM with attention"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T06:42:08.489551700Z",
     "start_time": "2026-01-22T06:42:08.455499400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "id": "1797becafce546f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1650\n",
      "Memory: 4.29 GB\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "b8e7629c",
   "metadata": {},
   "source": "# Building A Vocabulary"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T06:26:58.820969700Z",
     "start_time": "2026-01-22T06:26:58.806369100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=5):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "        self.add_word(\"<PAD>\")\n",
    "        self.add_word(\"<START>\")\n",
    "        self.add_word(\"<END>\")\n",
    "        self.add_word(\"<UNK>\")\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def build_vocabulary(self, caption_list):\n",
    "        frequencies = Counter()\n",
    "\n",
    "        for caption in caption_list:\n",
    "            tokens = self.tokenize(caption)\n",
    "            frequencies.update(tokens)\n",
    "\n",
    "        for word, count in frequencies.items():\n",
    "            if count >= self.freq_threshold:\n",
    "                self.add_word(word)\n",
    "\n",
    "        print(f\"Vocabulary built with {len(self)} words\")\n",
    "        print(f\"Words appearing >= {self.freq_threshold} times\")\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "\n",
    "        indices = [self.word2idx[\"<START>\"]]\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.word2idx:\n",
    "                indices.append(self.word2idx[token])\n",
    "            else:\n",
    "                indices.append(self.word2idx[\"<UNK>\"])\n",
    "\n",
    "        indices.append(self.word2idx[\"<END>\"])\n",
    "\n",
    "        return indices"
   ],
   "id": "61bc68767417718",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building A Dataset",
   "id": "680b5658e818f959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T06:33:08.893720400Z",
     "start_time": "2026-01-22T06:33:08.854132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, vocab=None, transform=None, build_vocab=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        print(f\"Loaded {len(self.df)} image-caption pairs\")\n",
    "\n",
    "        if build_vocab:\n",
    "            self.vocab = Vocabulary(freq_threshold=5)\n",
    "            self.vocab.build_vocabulary(self.df['caption'].tolist())\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        img_name = self.df.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.root_dir, 'Images', img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        numericalized_caption = self.vocab.numericalize(caption)\n",
    "\n",
    "        return image, torch.tensor(numericalized_caption)"
   ],
   "id": "9f16062c9c308a16",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding Custom Collate",
   "id": "f62186fc7a6249f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T06:42:17.382627Z",
     "start_time": "2026-01-22T06:42:17.355206900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images = [item[0] for item in batch]\n",
    "        captions = [item[1] for item in batch]\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        lengths = [len(cap) for cap in captions]\n",
    "\n",
    "        captions = pad_sequence(captions, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        return images, captions, lengths\n"
   ],
   "id": "e963e83f632052a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNN Architechture",
   "id": "861be7f545af9be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, encoded_image_size=7):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        # 3 -> 64\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 224 -> 112\n",
    "        )\n",
    "\n",
    "        # 64 -> 128\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 112 -> 56\n",
    "        )\n",
    "\n",
    "        # 128 -> 256\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 56 -> 28\n",
    "        )\n",
    "\n",
    "        # 256 -> 512\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 28 -> 14\n",
    "        )\n",
    "\n",
    "        # 512 -> 512\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 14 -> 7\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.block1(images)  # (batch, 64, 112, 112)\n",
    "        x = self.block2(x) # (batch, 128, 56, 56)\n",
    "        x = self.block3(x) # (batch, 256, 28, 28)\n",
    "        x = self.block4(x) # (batch, 512, 14, 14)\n",
    "        x = self.block5(x) # (batch, 512, 7, 7)\n",
    "\n",
    "        # For attention: (batch, 512, 7, 7) -> (batch, 49, 512)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.permute(0, 2, 3, 1)  # (batch, 7, 7, 512)\n",
    "        x = x.view(batch_size, -1, 512)  # (batch, 49, 512)\n",
    "\n",
    "        return x\n"
   ],
   "id": "c2e67a68dff2fe05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding The Bahdanau Attention",
   "id": "f6cb07fc69b6c993"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:02:31.307708Z",
     "start_time": "2026-01-22T07:02:31.290072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)\n",
    "        att2 = self.decoder_att(decoder_hidden)\n",
    "        att = self.relu(att1 + att2.unsqueeze(1))\n",
    "        att = self.full_att(att)\n",
    "        alpha = self.softmax(att.squeeze(2))\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "\n",
    "        return context, alpha"
   ],
   "id": "8137f2e0b4fde3c5",
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
