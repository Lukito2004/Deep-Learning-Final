{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b132cba1",
   "metadata": {},
   "source": [
    "# The Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0648fd1",
   "metadata": {},
   "source": [
    "I want to use the architecture encoder-decoder with attention architecture pattern:\n",
    "\n",
    "* Encode the image into a rich visual representation\n",
    "* Decode our representation into a sequence of words, one word at a time\n",
    "* Use attention to let the decoder focus on different image regions for each word\n",
    "\n",
    "Encoder will be a CNN with spatial feature map (7x7x512)\n",
    "Decoder will be an LSTM with attention"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:45:26.943925400Z",
     "start_time": "2026-01-22T07:45:24.244292300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "id": "1797becafce546f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1650\n",
      "Memory: 4.29 GB\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "b8e7629c",
   "metadata": {},
   "source": "# Building A Vocabulary"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T06:26:58.820969700Z",
     "start_time": "2026-01-22T06:26:58.806369100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=5):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "        self.add_word(\"<PAD>\")\n",
    "        self.add_word(\"<START>\")\n",
    "        self.add_word(\"<END>\")\n",
    "        self.add_word(\"<UNK>\")\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def build_vocabulary(self, caption_list):\n",
    "        frequencies = Counter()\n",
    "\n",
    "        for caption in caption_list:\n",
    "            tokens = self.tokenize(caption)\n",
    "            frequencies.update(tokens)\n",
    "\n",
    "        for word, count in frequencies.items():\n",
    "            if count >= self.freq_threshold:\n",
    "                self.add_word(word)\n",
    "\n",
    "        print(f\"Vocabulary built with {len(self)} words\")\n",
    "        print(f\"Words appearing >= {self.freq_threshold} times\")\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "\n",
    "        indices = [self.word2idx[\"<START>\"]]\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.word2idx:\n",
    "                indices.append(self.word2idx[token])\n",
    "            else:\n",
    "                indices.append(self.word2idx[\"<UNK>\"])\n",
    "\n",
    "        indices.append(self.word2idx[\"<END>\"])\n",
    "\n",
    "        return indices"
   ],
   "id": "61bc68767417718",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building A Dataset",
   "id": "680b5658e818f959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T06:33:08.893720400Z",
     "start_time": "2026-01-22T06:33:08.854132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, vocab=None, transform=None, build_vocab=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        print(f\"Loaded {len(self.df)} image-caption pairs\")\n",
    "\n",
    "        if build_vocab:\n",
    "            self.vocab = Vocabulary(freq_threshold=5)\n",
    "            self.vocab.build_vocabulary(self.df['caption'].tolist())\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.df.iloc[idx]['caption']\n",
    "        img_name = self.df.iloc[idx]['image']\n",
    "        img_path = os.path.join(self.root_dir, 'Images', img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        numericalized_caption = self.vocab.numericalize(caption)\n",
    "\n",
    "        return image, torch.tensor(numericalized_caption)"
   ],
   "id": "9f16062c9c308a16",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding Custom Collate",
   "id": "f62186fc7a6249f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T06:42:17.382627Z",
     "start_time": "2026-01-22T06:42:17.355206900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images = [item[0] for item in batch]\n",
    "        captions = [item[1] for item in batch]\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        lengths = [len(cap) for cap in captions]\n",
    "\n",
    "        captions = pad_sequence(captions, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        return images, captions, lengths\n"
   ],
   "id": "e963e83f632052a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNN Architechture",
   "id": "861be7f545af9be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, encoded_image_size=7):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        # 3 -> 64\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 224 -> 112\n",
    "        )\n",
    "\n",
    "        # 64 -> 128\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 112 -> 56\n",
    "        )\n",
    "\n",
    "        # 128 -> 256\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 56 -> 28\n",
    "        )\n",
    "\n",
    "        # 256 -> 512\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 28 -> 14\n",
    "        )\n",
    "\n",
    "        # 512 -> 512\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 14 -> 7\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.block1(images)  # (batch, 64, 112, 112)\n",
    "        x = self.block2(x) # (batch, 128, 56, 56)\n",
    "        x = self.block3(x) # (batch, 256, 28, 28)\n",
    "        x = self.block4(x) # (batch, 512, 14, 14)\n",
    "        x = self.block5(x) # (batch, 512, 7, 7)\n",
    "\n",
    "        # For attention: (batch, 512, 7, 7) -> (batch, 49, 512)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.permute(0, 2, 3, 1)  # (batch, 7, 7, 512)\n",
    "        x = x.view(batch_size, -1, 512)  # (batch, 49, 512)\n",
    "\n",
    "        return x\n"
   ],
   "id": "c2e67a68dff2fe05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding The Bahdanau Attention",
   "id": "f6cb07fc69b6c993"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:02:31.307708Z",
     "start_time": "2026-01-22T07:02:31.290072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)\n",
    "        att2 = self.decoder_att(decoder_hidden)\n",
    "        att = self.relu(att1 + att2.unsqueeze(1))\n",
    "        att = self.full_att(att)\n",
    "        alpha = self.softmax(att.squeeze(2))\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "\n",
    "        return context, alpha"
   ],
   "id": "8137f2e0b4fde3c5",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LSTM Decoder With Attention",
   "id": "7935129ae5f4a3ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:23:13.495187400Z",
     "start_time": "2026-01-22T07:23:13.434396300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size,\n",
    "                 encoder_dim=512, dropout=0.25):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = BahdanauAttention(encoder_dim, decoder_dim, attention_dim)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self.lstm_cell = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim)\n",
    "\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "\n",
    "        h = self.init_h(mean_encoder_out)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, captions, lengths):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        lengths_sorted, sort_idx = torch.sort(torch.tensor(lengths), descending=True)\n",
    "        encoder_out = encoder_out[sort_idx]\n",
    "        captions = captions[sort_idx]\n",
    "\n",
    "        embeddings = self.embedding(captions)\n",
    "\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "\n",
    "        decode_lengths = (lengths_sorted - 1).tolist()\n",
    "        max_length = max(decode_lengths)\n",
    "\n",
    "        predictions = torch.zeros(batch_size, max_length, self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max_length, num_pixels).to(device)\n",
    "\n",
    "        for t in range(max_length):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "\n",
    "            context, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "\n",
    "            lstm_input = torch.cat([embeddings[:batch_size_t, t, :], context], dim=1)\n",
    "            h_t, c_t = self.lstm_cell(lstm_input, (h[:batch_size_t], c[:batch_size_t]))\n",
    "\n",
    "            preds = self.fc(self.dropout_layer(h_t))\n",
    "\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "            h = h_t\n",
    "            c = c_t\n",
    "\n",
    "        return predictions, alphas, sort_idx"
   ],
   "id": "3a1dc2e61bfd8837",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Complete Model",
   "id": "bac8de471b886f6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:44:40.813899200Z",
     "start_time": "2026-01-22T07:44:40.798748800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, attention_dim=512, embed_dim=256,\n",
    "                 decoder_dim=512, encoder_dim=512, dropout=0.25):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "\n",
    "        self.encoder = CNNEncoder()\n",
    "        self.decoder = LSTMDecoder(\n",
    "            attention_dim=attention_dim,\n",
    "            embed_dim=embed_dim,\n",
    "            decoder_dim=decoder_dim,\n",
    "            vocab_size=vocab_size,\n",
    "            encoder_dim=encoder_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, images, captions, lengths):\n",
    "        encoder_out = self.encoder(images)\n",
    "        predictions, alphas, sort_idx = self.decoder(encoder_out, captions, lengths)\n",
    "        return predictions, alphas, sort_idx"
   ],
   "id": "94e4eefb6b973d62",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Setup",
   "id": "c1f0ebd14a6ff72a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T07:45:34.577307Z",
     "start_time": "2026-01-22T07:45:34.561528900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(root_dir='./caption_data', batch_size=32, num_workers=4):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"Building vocabulary...\")\n",
    "    full_dataset = FlickrDataset(\n",
    "        root_dir=root_dir,\n",
    "        captions_file=os.path.join(root_dir, 'captions.txt'),\n",
    "        transform=transform,\n",
    "        build_vocab=True\n",
    "    )\n",
    "\n",
    "    vocab = full_dataset.vocab\n",
    "\n",
    "    dataset_size = len(full_dataset)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    val_size = int(0.1 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    print(f\"Dataset split:\")\n",
    "    print(f\"Train: {len(train_dataset)} samples\")\n",
    "    print(f\"Val: {len(val_dataset)} samples\")\n",
    "    print(f\"Test: {len(test_dataset)} samples\")\n",
    "\n",
    "    pad_idx = vocab.word2idx[\"<PAD>\"]\n",
    "    collate_fn = CustomCollate(pad_idx=pad_idx)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, vocab"
   ],
   "id": "a833689fce91a2ca",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Loop",
   "id": "4dd267140a4fb9b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, vocab, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pad_idx = vocab.word2idx[\"<PAD>\"]\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for batch_idx, (images, captions, lengths) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        predictions, alphas, sort_idx = model(images, captions, lengths)\n",
    "\n",
    "        targets = captions[:, 1:]  # Remove <START>\n",
    "\n",
    "        predictions = predictions.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "\n",
    "        predictions = predictions.view(-1, predictions.size(-1))\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss"
   ],
   "id": "ae67eb0d4a145d13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def validate(model, val_loader, criterion, vocab):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pad_idx = vocab.word2idx[\"<PAD>\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in tqdm(val_loader, desc='Validation'):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            predictions, alphas, sort_idx = model(images, captions, lengths)\n",
    "\n",
    "            targets = captions[:, 1:]  # Remove <START>\n",
    "\n",
    "            predictions = predictions.contiguous().view(-1, predictions.size(-1))\n",
    "            targets = targets.contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss"
   ],
   "id": "45c7bf85e3e8d5fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_sample_captions(model, val_loader, vocab, num_samples=5):\n",
    "    model.eval()\n",
    "    samples_shown = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in val_loader:\n",
    "            if samples_shown >= num_samples:\n",
    "                break\n",
    "\n",
    "            image = images[0].unsqueeze(0).to(device)\n",
    "            caption_gt = captions[0]\n",
    "\n",
    "            encoder_out = model.encoder(image)\n",
    "\n",
    "            generated_caption = []\n",
    "            h, c = model.decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "            current_word = torch.tensor([vocab.word2idx['<START>']]).to(device)\n",
    "\n",
    "            max_length = 20\n",
    "            for _ in range(max_length):\n",
    "                word_emb = model.decoder.embedding(current_word)\n",
    "\n",
    "                context, alpha = model.decoder.attention(encoder_out, h)\n",
    "\n",
    "                lstm_input = torch.cat([word_emb, context], dim=1)\n",
    "                h, c = model.decoder.lstm_cell(lstm_input, (h, c))\n",
    "\n",
    "                logits = model.decoder.fc(h)\n",
    "                predicted_word_idx = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "                if vocab.idx2word[predicted_word_idx] == '<END>':\n",
    "                    break\n",
    "\n",
    "                generated_caption.append(vocab.idx2word[predicted_word_idx])\n",
    "                current_word = torch.tensor([predicted_word_idx]).to(device)\n",
    "\n",
    "            gt_caption = [vocab.idx2word[idx.item()] for idx in caption_gt\n",
    "                         if vocab.idx2word[idx.item()] not in ['<START>', '<END>', '<PAD>']]\n",
    "\n",
    "            print(f\"Sample {samples_shown + 1}:\")\n",
    "            print(f\"Generated: {' '.join(generated_caption)}\")\n",
    "            print(f\"Ground Truth: {' '.join(gt_caption)}\")\n",
    "\n",
    "            samples_shown += 1"
   ],
   "id": "92bab322b21d4849"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d7776e2332d83ba3"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
