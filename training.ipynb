{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b132cba1",
   "metadata": {},
   "source": [
    "# The Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0648fd1",
   "metadata": {},
   "source": [
    "I want to use the architecture encoder-decoder with attention architecture pattern:\n",
    "\n",
    "* Encode the image into a rich visual representation\n",
    "* Decode our representation into a sequence of words, one word at a time\n",
    "* Use attention to let the decoder focus on different image regions for each word\n",
    "\n",
    "Encoder will be a CNN with spatial feature map (7x7x512)\n",
    "Decoder will be an LSTM with attention"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.203058Z",
     "start_time": "2026-01-22T09:07:46.097051400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import cv2\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ],
   "id": "1797becafce546f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1650\n",
      "Memory: 4.29 GB\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "b8e7629c",
   "metadata": {},
   "source": "# Building A Vocabulary"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.251374600Z",
     "start_time": "2026-01-22T09:07:49.218956400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=5):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "        self.add_word(\"<PAD>\")\n",
    "        self.add_word(\"<START>\")\n",
    "        self.add_word(\"<END>\")\n",
    "        self.add_word(\"<UNK>\")\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def build_vocabulary(self, caption_list):\n",
    "        frequencies = Counter()\n",
    "\n",
    "        for caption in caption_list:\n",
    "            tokens = self.tokenize(caption)\n",
    "            frequencies.update(tokens)\n",
    "\n",
    "        for word, count in frequencies.items():\n",
    "            if count >= self.freq_threshold:\n",
    "                self.add_word(word)\n",
    "\n",
    "        print(f\"Vocabulary built with {len(self)} words\")\n",
    "        print(f\"Words appearing >= {self.freq_threshold} times\")\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokens = self.tokenize(text)\n",
    "\n",
    "        indices = [self.word2idx[\"<START>\"]]\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in self.word2idx:\n",
    "                indices.append(self.word2idx[token])\n",
    "            else:\n",
    "                indices.append(self.word2idx[\"<UNK>\"])\n",
    "\n",
    "        indices.append(self.word2idx[\"<END>\"])\n",
    "\n",
    "        return indices"
   ],
   "id": "61bc68767417718",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vocabulary is a much needed thing for our model, with it we can numericalize/tokenize whatever sentence we want. We have freq_threshold which tells us the minimum number of times a word needs to be repeated to be in this vocabulary. I have a mapping of each word to it's unique index and also from each index to its unique word.\n",
    "\n",
    "During numericalization I start the sentence with <START> and end it with <END>, if this sentence doesn't have a word that repeats at least 5 times, I fill it with <UNK> instead."
   ],
   "id": "eead69f6cca12ca9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building A Dataset",
   "id": "680b5658e818f959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.301947Z",
     "start_time": "2026-01-22T09:07:49.267124200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, vocab=None, transform=None, build_vocab=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        print(f\"Loaded {len(self.df)} image-caption pairs\")\n",
    "\n",
    "        # Precompute image paths\n",
    "        self.image_paths = [\n",
    "            os.path.join(self.root_dir, 'Images', img_name)\n",
    "            for img_name in self.df['image']\n",
    "        ]\n",
    "\n",
    "        # Precompute captions\n",
    "        self.captions = self.df['caption'].tolist()\n",
    "\n",
    "        if build_vocab:\n",
    "            self.vocab = Vocabulary(freq_threshold=5)\n",
    "            self.vocab.build_vocabulary(self.captions)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        caption = self.captions[idx]\n",
    "\n",
    "        try:\n",
    "            image = cv2.imread(img_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(image)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "        numericalized_caption = self.vocab.numericalize(caption)\n",
    "\n",
    "        return image, torch.tensor(numericalized_caption, dtype=torch.long)"
   ],
   "id": "9f16062c9c308a16",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This dataset was much needed for my encoder-decoder pattern, its __getitem__ method returns (image_tensor, caption_tensor).\n",
    "\n",
    "This dataset creates a 5 frequency dataset and after that it build that vocabulary with our captions.\n",
    "\n",
    "The Dataset loads the images and applies the transform if any exist, then numericalizes the caption and returns the (image_tensor, caption_tensor)"
   ],
   "id": "cb7aecda52e0a6d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adding Custom Collate",
   "id": "f62186fc7a6249f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.341786300Z",
     "start_time": "2026-01-22T09:07:49.312346400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images = [item[0] for item in batch]\n",
    "        captions = [item[1] for item in batch]\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "\n",
    "        lengths = [len(cap) for cap in captions]\n",
    "\n",
    "        captions = pad_sequence(captions, batch_first=True, padding_value=self.pad_idx)\n",
    "\n",
    "        return images, captions, lengths\n"
   ],
   "id": "e963e83f632052a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This CustomCollate class is needed for collate_fn for PyTorch's dataloader, with this we imitate each caption as same length with the use of <PAD> token.",
   "id": "5aa8796b003a1058"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CNN Architechture",
   "id": "861be7f545af9be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.385469Z",
     "start_time": "2026-01-22T09:07:49.357706300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, encoded_image_size=7):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "\n",
    "        self.enc_image_size = encoded_image_size\n",
    "\n",
    "        # 3 -> 64\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 224 -> 112\n",
    "        )\n",
    "\n",
    "        # 64 -> 128\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 112 -> 56\n",
    "        )\n",
    "\n",
    "        # 128 -> 256\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 56 -> 28\n",
    "        )\n",
    "\n",
    "        # 256 -> 512\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 28 -> 14\n",
    "        )\n",
    "\n",
    "        # 512 -> 512\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)  # 14 -> 7\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.block1(images)  # (batch, 64, 112, 112)\n",
    "        x = self.block2(x) # (batch, 128, 56, 56)\n",
    "        x = self.block3(x) # (batch, 256, 28, 28)\n",
    "        x = self.block4(x) # (batch, 512, 14, 14)\n",
    "        x = self.block5(x) # (batch, 512, 7, 7)\n",
    "\n",
    "        # For attention: (batch, 512, 7, 7) -> (batch, 49, 512)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.permute(0, 2, 3, 1)  # (batch, 7, 7, 512)\n",
    "        x = x.view(batch_size, -1, 512)  # (batch, 49, 512)\n",
    "\n",
    "        return x\n"
   ],
   "id": "c2e67a68dff2fe05",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This CNN encoder Architecture is one of the core things in my project, I use it to encode images into a set of visual features that later my decoder can look at to generate words.\n",
    "\n",
    "In essence the CNN converts the image (batch, 3, 224, 224) to features (batch, 49, 512)\n",
    "\n",
    "* Each image is split into 7x7 areas with 49 feature vectors\n",
    "* Each vector has 512 channels\n",
    "\n",
    "This type of output sets us up attention-based decoding\n",
    "\n",
    "* Block 1:\n",
    "\n",
    "Converts (batch, 3, 224, 224) to (batch, 64, 112, 112)\n",
    "\n",
    "* Block 2:\n",
    "\n",
    "Converts (batch, 64, 112, 112) to (batch, 128, 56, 56)\n",
    "\n",
    "* Block 3:\n",
    "\n",
    "Converts (batch, 128, 56, 56) to (batch, 256, 28, 28)\n",
    "\n",
    "* Block 4:\n",
    "\n",
    "Converts (batch, 256, 28, 28) to (batch, 256, 14, 14)\n",
    "\n",
    "* Block 5:\n",
    "\n",
    "Converts (batch, 256, 14, 14) to (batch, 512, 7, 7)\n",
    "\n",
    "It does all of this in this style:\n",
    "\n",
    "Convolution -> Batch Normalization -> ReLU -> Convolution -> Batch Normalization -> ReLU -> Max Pooling\n",
    "\n",
    "I decided to use convolution twice to add more non linearity, I use batch normalization so that the CNN doesn't get freaked out when I show it at first a human and then a dog, I use max pooling to put importance on the most important features only\n",
    "\n",
    "During weight initialization I use kaiming_normal which is the pytorch way of He initialization which is designed for ReLU activation functions\n",
    "\n",
    "In forward propagation I of course put these images through these blocks to get the feature vector, then I prepare it for attention, so instead of having (batch, 512, 7, 7) we convert it into (batch, 49, 512), this clearly outputs that there are 49 areas of focus in the image with each having 512D feature vector.\n"
   ],
   "id": "e75316d941498c5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.432589500Z",
     "start_time": "2026-01-22T09:07:49.401356300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
    "\n",
    "        self.full_att = nn.Linear(attention_dim, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)\n",
    "        att2 = self.decoder_att(decoder_hidden)\n",
    "        att = self.relu(att1 + att2.unsqueeze(1))\n",
    "        att = self.full_att(att)\n",
    "        alpha = self.softmax(att.squeeze(2))\n",
    "        context = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)\n",
    "\n",
    "        return context, alpha"
   ],
   "id": "8137f2e0b4fde3c5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I Use BahdanauAttention to implement additive attention to decide on which area to focus on each word.\n",
    "\n",
    "* Our encoder_att transforms encoder features to attention space\n",
    "* decoder_att transforms decoder hidden state into attention space\n",
    "* full_att gives us a single score of the attention\n",
    "* ReLU introduces non linearity as always\n",
    "* Softmax functions converts scores into probabilities\n",
    "\n",
    "The Forward function at first calculates two variables att1 and att2.\n",
    "\n",
    "* att1 is calculated with encoder_att "
   ],
   "id": "e046b1749f9d8c62"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LSTM Decoder With Attention",
   "id": "7935129ae5f4a3ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.497774700Z",
     "start_time": "2026-01-22T09:07:49.432589500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size,\n",
    "                 encoder_dim=512, dropout=0.25):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = BahdanauAttention(encoder_dim, decoder_dim, attention_dim)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "\n",
    "        self.lstm_cell = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim)\n",
    "\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n",
    "\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "\n",
    "        h = self.init_h(mean_encoder_out)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, captions, lengths):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        lengths_sorted, sort_idx = torch.sort(torch.tensor(lengths), descending=True)\n",
    "        encoder_out = encoder_out[sort_idx]\n",
    "        captions = captions[sort_idx]\n",
    "\n",
    "        embeddings = self.embedding(captions)\n",
    "        h, c = self.init_hidden_state(encoder_out)\n",
    "\n",
    "        decode_lengths = (lengths_sorted - 1).tolist()\n",
    "        max_length = max(decode_lengths)\n",
    "\n",
    "        predictions = torch.zeros(batch_size, max_length, self.vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max_length, num_pixels).to(device)\n",
    "\n",
    "        for t in range(max_length):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            context, alpha = self.attention(encoder_out[:batch_size_t], h[:batch_size_t])\n",
    "            lstm_input = torch.cat([embeddings[:batch_size_t, t, :], context], dim=1)\n",
    "            h_t, c_t = self.lstm_cell(lstm_input, (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(self.dropout_layer(h_t))\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "            h = h_t\n",
    "            c = c_t\n",
    "\n",
    "        unsort_idx = torch.argsort(sort_idx)\n",
    "        predictions = predictions[unsort_idx]\n",
    "        alphas = alphas[unsort_idx]\n",
    "        captions = captions[unsort_idx]\n",
    "\n",
    "        return predictions, alphas, captions"
   ],
   "id": "3a1dc2e61bfd8837",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Complete Model",
   "id": "bac8de471b886f6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.532356300Z",
     "start_time": "2026-01-22T09:07:49.503025500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, vocab_size, attention_dim=512, embed_dim=256,\n",
    "                 decoder_dim=512, encoder_dim=512, dropout=0.25):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "\n",
    "        self.encoder = CNNEncoder()\n",
    "        self.decoder = LSTMDecoder(\n",
    "            attention_dim=attention_dim,\n",
    "            embed_dim=embed_dim,\n",
    "            decoder_dim=decoder_dim,\n",
    "            vocab_size=vocab_size,\n",
    "            encoder_dim=encoder_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, images, captions, lengths):\n",
    "        encoder_out = self.encoder(images)\n",
    "        predictions, alphas, sort_idx = self.decoder(encoder_out, captions, lengths)\n",
    "        return predictions, alphas, sort_idx"
   ],
   "id": "94e4eefb6b973d62",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Setup",
   "id": "c1f0ebd14a6ff72a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.577088800Z",
     "start_time": "2026-01-22T09:07:49.548168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(root_dir='./caption_data', batch_size=32, num_workers=4, persistent_workers=True):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"Building vocabulary...\")\n",
    "    full_dataset = FlickrDataset(\n",
    "        root_dir=root_dir,\n",
    "        captions_file=os.path.join(root_dir, 'captions.txt'),\n",
    "        transform=transform,\n",
    "        build_vocab=True\n",
    "    )\n",
    "\n",
    "    vocab = full_dataset.vocab\n",
    "\n",
    "    dataset_size = len(full_dataset)\n",
    "    train_size = int(0.8 * dataset_size)\n",
    "    val_size = int(0.1 * dataset_size)\n",
    "    test_size = dataset_size - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    print(f\"Dataset split:\")\n",
    "    print(f\"Train: {len(train_dataset)} samples\")\n",
    "    print(f\"Val: {len(val_dataset)} samples\")\n",
    "    print(f\"Test: {len(test_dataset)} samples\")\n",
    "\n",
    "    pad_idx = vocab.word2idx[\"<PAD>\"]\n",
    "    collate_fn = CustomCollate(pad_idx=pad_idx)\n",
    "\n",
    "    # Set prefetch_factor and persistent_workers based on num_workers\n",
    "    dataloader_kwargs = {\n",
    "        'batch_size': batch_size,\n",
    "        'collate_fn': collate_fn,\n",
    "        'pin_memory': True,\n",
    "    }\n",
    "\n",
    "    if num_workers > 0:\n",
    "        dataloader_kwargs['num_workers'] = num_workers\n",
    "        dataloader_kwargs['persistent_workers'] = persistent_workers\n",
    "        dataloader_kwargs['prefetch_factor'] = 2\n",
    "    else:\n",
    "        dataloader_kwargs['num_workers'] = 0\n",
    "        # Don't set prefetch_factor or persistent_workers when num_workers=0\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        **dataloader_kwargs\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        shuffle=False,\n",
    "        **dataloader_kwargs\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        shuffle=False,\n",
    "        **dataloader_kwargs\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader, vocab"
   ],
   "id": "a833689fce91a2ca",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training Loop",
   "id": "4dd267140a4fb9b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.620980900Z",
     "start_time": "2026-01-22T09:07:49.592879400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, vocab, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pad_idx = vocab.word2idx[\"<PAD>\"]\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    for batch_idx, (images, captions, lengths) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        predictions, alphas, sort_idx = model(images, captions, lengths)\n",
    "\n",
    "        targets = captions[:, 1:]  # Remove <START>\n",
    "\n",
    "        predictions = predictions.contiguous()\n",
    "        targets = targets.contiguous()\n",
    "\n",
    "        predictions = predictions.view(-1, predictions.size(-1))\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss"
   ],
   "id": "ae67eb0d4a145d13",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.678295Z",
     "start_time": "2026-01-22T09:07:49.636901800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate(model, val_loader, criterion, vocab):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    pad_idx = vocab.word2idx[\"<PAD>\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in tqdm(val_loader, desc='Validation'):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            predictions, alphas, sort_idx = model(images, captions, lengths)\n",
    "\n",
    "            targets = captions[:, 1:]  # Remove <START>\n",
    "\n",
    "            predictions = predictions.contiguous().view(-1, predictions.size(-1))\n",
    "            targets = targets.contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(predictions, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss"
   ],
   "id": "45c7bf85e3e8d5fe",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.722175500Z",
     "start_time": "2026-01-22T09:07:49.679312400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_sample_captions(model, val_loader, vocab, num_samples=5, temperature=1.0):\n",
    "    model.eval()\n",
    "    samples_shown = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, captions, lengths in val_loader:\n",
    "            if samples_shown >= num_samples:\n",
    "                break\n",
    "\n",
    "            image = images[0].unsqueeze(0).to(device)\n",
    "            caption_gt = captions[0]\n",
    "\n",
    "            encoder_out = model.encoder(image)\n",
    "\n",
    "            generated_caption = []\n",
    "            h, c = model.decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "            current_word = torch.tensor([vocab.word2idx['<START>']]).to(device)\n",
    "\n",
    "            max_length = 20\n",
    "            for _ in range(max_length):\n",
    "                word_emb = model.decoder.embedding(current_word)\n",
    "                context, alpha = model.decoder.attention(encoder_out, h)\n",
    "                lstm_input = torch.cat([word_emb, context], dim=1)\n",
    "                h, c = model.decoder.lstm_cell(lstm_input, (h, c))\n",
    "\n",
    "                logits = model.decoder.fc(h)\n",
    "\n",
    "                # Apply temperature and add small noise to break ties\n",
    "                logits = logits / temperature\n",
    "                logits = logits + torch.randn_like(logits) * 0.01  # Small noise\n",
    "\n",
    "                predicted_word_idx = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "                if vocab.idx2word[predicted_word_idx] == '<END>':\n",
    "                    break\n",
    "\n",
    "                generated_caption.append(vocab.idx2word[predicted_word_idx])\n",
    "                current_word = torch.tensor([predicted_word_idx]).to(device)\n",
    "\n",
    "            gt_caption = [vocab.idx2word[idx.item()] for idx in caption_gt\n",
    "                         if vocab.idx2word[idx.item()] not in ['<START>', '<END>', '<PAD>']]\n",
    "\n",
    "            print(f\"Sample {samples_shown + 1}:\")\n",
    "            print(f\"Generated: {' '.join(generated_caption)}\")\n",
    "            print(f\"Ground Truth: {' '.join(gt_caption)}\")\n",
    "\n",
    "            samples_shown += 1"
   ],
   "id": "92bab322b21d4849",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, ignore_index, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.smoothing = smoothing\n",
    "        self.confidence = 1.0 - smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "\n",
    "        # Mask padding\n",
    "        mask = (target != self.ignore_index).float()\n",
    "        nll_loss = (nll_loss * mask).sum() / mask.sum()\n",
    "        smooth_loss = (smooth_loss * mask).sum() / mask.sum()\n",
    "\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss"
   ],
   "id": "302a9ae1884b2700"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:07:49.760902500Z",
     "start_time": "2026-01-22T09:07:49.726896200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model, train_loader, val_loader, vocab, num_epochs=20,\n",
    "                learning_rate=3e-4, save_path='best_model.pth'):\n",
    "    pad_idx = vocab.word2idx[\"<PAD>\"]\n",
    "    criterion = LabelSmoothingCrossEntropy(ignore_index=pad_idx, smoothing=0.1)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience_limit = 5  # Early stopping\n",
    "\n",
    "    print(f\"Starting training on {device}\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print()\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, vocab, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        val_loss = validate(model, val_loader, criterion, vocab)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        print()\n",
    "        print(f\"Epoch {epoch} Summary:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        print()\n",
    "        print(\"Sample captions:\")\n",
    "        generate_sample_captions(model, val_loader, vocab, num_samples=3)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'vocab': vocab\n",
    "            }\n",
    "            torch.save(checkpoint, save_path)\n",
    "            print(f\"Model saved with val_loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience: {patience_counter}/{patience_limit}\")\n",
    "\n",
    "        if patience_counter >= patience_limit:\n",
    "            print()\n",
    "            print(f\"Early stopping triggered after {epoch} epochs\")\n",
    "            break\n",
    "\n",
    "    print()\n",
    "    print(f\"Training complete!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses"
   ],
   "id": "90458f4d03e12c3b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-22T09:10:29.817794400Z",
     "start_time": "2026-01-22T09:07:49.776799500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 1e-4\n",
    "    EMBED_DIM = 256\n",
    "    ATTENTION_DIM = 512\n",
    "    DECODER_DIM = 512\n",
    "    ENCODER_DIM = 512\n",
    "    DROPOUT = 0.35\n",
    "\n",
    "    print(\"Preparing data...\")\n",
    "    train_loader, val_loader, test_loader, vocab = prepare_data(\n",
    "        root_dir='./caption_data',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=4,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    print()\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    # Create model\n",
    "    print()\n",
    "    print(\"Creating model...\")\n",
    "    model = ImageCaptioningModel(\n",
    "        vocab_size=len(vocab),\n",
    "        attention_dim=ATTENTION_DIM,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        decoder_dim=DECODER_DIM,\n",
    "        encoder_dim=ENCODER_DIM,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "\n",
    "    print()\n",
    "    print(\"Model Architecture:\")\n",
    "    print(model)\n",
    "    print()\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # Train model\n",
    "    train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        vocab=vocab,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        save_path='best_model.pth'\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print()\n",
    "    print(\"Training complete! Model saved as 'best_model.pth'\")\n",
    "    print(\"Training curves saved as 'training_curves.png'\")\n",
    "\n",
    "    print()\n",
    "    print(\"Saving vocabulary...\")\n",
    "    with open('vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(\"Vocabulary saved as 'vocab.pkl'\")"
   ],
   "id": "531f38388a150355",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Building vocabulary...\n",
      "Loaded 40455 image-caption pairs\n",
      "Vocabulary built with 3005 words\n",
      "Words appearing >= 5 times\n",
      "Dataset split:\n",
      "Train: 32364 samples\n",
      "Val: 4045 samples\n",
      "Test: 4046 samples\n",
      "\n",
      "Vocabulary size: 3005\n",
      "\n",
      "Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukito\\Desktop\\Finals\\Deep-Learning-Final\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "ImageCaptioningModel(\n",
      "  (encoder): CNNEncoder(\n",
      "    (block1): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (block4): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (block5): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): LSTMDecoder(\n",
      "    (attention): BahdanauAttention(\n",
      "      (encoder_att): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (decoder_att): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (full_att): Linear(in_features=512, out_features=1, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (softmax): Softmax(dim=1)\n",
      "    )\n",
      "    (embedding): Embedding(3005, 256)\n",
      "    (dropout_layer): Dropout(p=0.25, inplace=False)\n",
      "    (lstm_cell): LSTMCell(768, 512)\n",
      "    (init_h): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (init_c): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (fc): Linear(in_features=512, out_features=3005, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 13,037,566\n",
      "Trainable parameters: 13,037,566\n",
      "Starting training on cuda\n",
      "Model parameters: 13,037,566\n",
      "\n",
      "Epoch 1/20\n",
      "Learning rate: 0.000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|â–Ž         | 27/1012 [02:38<1:36:31,  5.88s/it, loss=6.06]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 41\u001B[39m\n\u001B[32m     38\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTrainable parameters: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28msum\u001B[39m(p.numel()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mp\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mmodel.parameters()\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mif\u001B[39;00m\u001B[38;5;250m \u001B[39mp.requires_grad)\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     40\u001B[39m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m41\u001B[39m train_losses, val_losses = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     42\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     44\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     45\u001B[39m \u001B[43m    \u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mNUM_EPOCHS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     47\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mLEARNING_RATE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     48\u001B[39m \u001B[43m    \u001B[49m\u001B[43msave_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbest_model.pth\u001B[39;49m\u001B[33;43m'\u001B[39;49m\n\u001B[32m     49\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m plt.figure(figsize=(\u001B[32m10\u001B[39m, \u001B[32m5\u001B[39m))\n\u001B[32m     52\u001B[39m plt.plot(train_losses, label=\u001B[33m'\u001B[39m\u001B[33mTrain Loss\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 26\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, val_loader, vocab, num_epochs, learning_rate, save_path)\u001B[39m\n\u001B[32m     23\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     24\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mLearning rate: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moptimizer.param_groups[\u001B[32m0\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mlr\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.6f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m train_loss = \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m train_losses.append(train_loss)\n\u001B[32m     29\u001B[39m val_loss = validate(model, val_loader, criterion, vocab)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 30\u001B[39m, in \u001B[36mtrain_epoch\u001B[39m\u001B[34m(model, train_loader, criterion, optimizer, vocab, epoch)\u001B[39m\n\u001B[32m     26\u001B[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001B[32m5.0\u001B[39m)\n\u001B[32m     28\u001B[39m     optimizer.step()\n\u001B[32m---> \u001B[39m\u001B[32m30\u001B[39m     total_loss += \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m     pbar.set_postfix({\u001B[33m'\u001B[39m\u001B[33mloss\u001B[39m\u001B[33m'\u001B[39m: loss.item()})\n\u001B[32m     33\u001B[39m avg_loss = total_loss / \u001B[38;5;28mlen\u001B[39m(train_loader)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "test_samples = 0\n",
    "with torch.no_grad():\n",
    "    for images, captions, lengths in test_loader:\n",
    "        if test_samples >= 5:\n",
    "            break\n",
    "\n",
    "        images = images.to(device)\n",
    "\n",
    "        image = images[0:1]\n",
    "        caption_gt = captions[0]\n",
    "\n",
    "        encoder_out = model.encoder(image)\n",
    "        h, c = model.decoder.init_hidden_state(encoder_out)\n",
    "        current_word = torch.tensor([vocab.word2idx['<START>']]).to(device)\n",
    "        generated = []\n",
    "\n",
    "        for _ in range(20):\n",
    "            word_emb = model.decoder.embedding(current_word)\n",
    "            context, alpha = model.decoder.attention(encoder_out, h)\n",
    "            lstm_input = torch.cat([word_emb, context], dim=1)\n",
    "            h, c = model.decoder.lstm_cell(lstm_input, (h, c))\n",
    "            logits = model.decoder.fc(h)\n",
    "            predicted_word_idx = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "            if vocab.idx2word[predicted_word_idx] == '<END>':\n",
    "                break\n",
    "\n",
    "            generated.append(vocab.idx2word[predicted_word_idx])\n",
    "            current_word = torch.tensor([predicted_word_idx]).to(device)\n",
    "\n",
    "        gt_words = [vocab.idx2word[idx.item()] for idx in caption_gt\n",
    "                   if vocab.idx2word[idx.item()] not in ['<START>', '<END>', '<PAD>']]\n",
    "\n",
    "        print()\n",
    "        print(f\"Test Sample {test_samples + 1}:\")\n",
    "        print(f\"Generated: {' '.join(generated)}\")\n",
    "        print(f\"Reference: {' '.join(gt_words)}\")\n",
    "        print()\n",
    "\n",
    "        test_samples += 1\n",
    "\n",
    "print()\n",
    "print(\"All done! Your model is ready for inference.\")"
   ],
   "id": "6002d1e44761ef6b"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
